{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a27b681d-1350-426f-94a3-050e26207d8b",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to finetune BERT model for article spoiling task. <br>\n",
    "Use the conda env: eda_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d283bc74",
   "metadata": {},
   "source": [
    "What do I want to do? <br>\n",
    "I would like to check pre-trained roberta and its generated text. Input should be tokenized. If output will be not maching, then I would like to fine-tune the model.\n",
    "\n",
    "Problems encountered: <br>\n",
    "BERT based models can process the ninput of 512 tokens maximum, the average of article body text is ~1600 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0c10af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# BERT model\n",
    "from transformers import RobertaTokenizer, RobertaForQuestionAnswering,  BertTokenizer, BertModel\n",
    "import torch\n",
    "import nltk\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "af85298a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wojom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# VARIABLES\n",
    "nltk.download('punkt')\n",
    "RANDOM_STATE = 42 # Random state for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9333de52",
   "metadata": {},
   "source": [
    "### Loading data and splitting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f80f950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3358, 6)\n"
     ]
    }
   ],
   "source": [
    "spoil_df = pd.read_csv(\"../data/spoiling_data.csv\", sep=\";\")\n",
    "print(spoil_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "618bff39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['targetTitle', 'targetParagraphs', 'humanSpoiler', 'spoiler', 'tags',\n",
       "       'spoilerPositions'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spoil_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ac2db47",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    spoil_df.drop(columns=[\"humanSpoiler\", \"spoiler\"]), \n",
    "    spoil_df[[\"humanSpoiler\", \"spoiler\"]],\n",
    "    test_size=0.2, \n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "x_test, x_val, y_test, y_val = train_test_split(\n",
    "    x_test, \n",
    "    y_test,\n",
    "    test_size=0.5,  # 50% of the original x_test size for validation\n",
    "    random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d9ad74",
   "metadata": {},
   "source": [
    "### Loading models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "088751a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_bert_uncased = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model_bert_uncased = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def calculate_bert_similarity(texts):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        inputs = tokenizer_bert_uncased(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        outputs = model_bert_uncased(**inputs)\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().detach().numpy()\n",
    "        embeddings.append(embedding)\n",
    "    \n",
    "    similarity = np.dot(embeddings[0], embeddings[1]) / (np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1]))\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c395f1",
   "metadata": {},
   "source": [
    "### Q&A with roBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ff6466",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_roberta = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model_roberta = RobertaForQuestionAnswering.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "18148dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = x_train.iloc[1][\"targetTitle\"]\n",
    "context = x_train.iloc[1][\"targetParagraphs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "46484cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(context)\n",
    "tokenized_corpus = [word_tokenize(doc.lower()) for doc in sentences]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "tokenized_query = word_tokenize(query.lower())\n",
    "scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "scores_idx = [[score, idx] for idx, score in enumerate(scores)]\n",
    "scores_idx.sort(reverse=True, key=lambda x: x[0])\n",
    "sentences = pd.DataFrame(sentences)\n",
    "important_sentences = sentences.iloc[[arr[1] for arr in scores_idx[:10]],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "9e2ff0cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In its 2017 budget request, NASA asked Congress for $1.3 billion to build its next jumbo rocket. It is happened before, Garver said, with the the $100 billion International Space Station. Building SLS let us NASA keep its options open if the next president decides to look to lunar landings instead, something that Obama seemed to rule out in a 2010 speech. The SLS — a rocket to nowhere, as Cowing put it — fits this pattern neatly because it provides thousands of jobs in space states. Might SpaceX privately building a big rocket in the next two years, and sending something to Mars, make SLS look redundant to politicians, finally ending NASA’s long cycle of rocket-building jobs programs? Last week, despite years of fighting with the Obama Administration over its plans to explore an asteroid with the rocket, the Senate Appropriations Committee not only granted the request, but gave the space agency an extra $995 million to build it. One piece of NASA’s massive new rocket NASA / Via nasa.gov ID: 8562041, Space adventure fans might enjoy NASA’s latest saga, the story of a rocket to nowhere that Congress wants, very badly, built in Alabama. A surprise billion dollars may sound good. The real problem, former NASA official Scott Pace of George Washington University told BuzzFeed News, is that the Obama administration’s plans to fly astronauts to an asteroid and then Mars are not very interesting to international or commercial partners, who would rather return to the moon. Cuts also came to NASA’s space technology funding, a $130 million bite meant to keep aloft earth-observing satellites run out of Goddard Space Flight Center, based in Mikulski’s state of Maryland. '"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = \"\"\n",
    "for sentence in important_sentences[0]:\n",
    "    context += str(sentence) + \" \"\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94137aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input tokens:  374 \n",
      "Should be less than 512\n",
      "Extracted Spoiler:  a rocket to nowhere, as Cowing put it — fits this pattern neatly because it provides thousands of jobs in space states. Might SpaceX privately building a big rocket in the next two years, and sending something to Mars, make SLS look redundant to politicians, finally ending NASA’s long cycle of rocket-building jobs programs? Last week, despite years of fighting with the Obama Administration over its plans to explore an asteroid with the rocket, the Senate Appropriations Committee not only granted the request, but\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer_roberta(query, context, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "print(\"Number of input tokens: \", len(inputs['input_ids'][0]), \"\\nShould be less than 512\")\n",
    "model_roberta.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model_roberta(**inputs)\n",
    "    start_index = torch.argmax(outputs.start_logits)\n",
    "    end_index = torch.argmax(outputs.end_logits)\n",
    "\n",
    "answer = tokenizer_roberta.decode(inputs['input_ids'][0][start_index:end_index + 1])\n",
    "print(\"Extracted Spoiler:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "d849b07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Similarity Score:  0.86259645\n"
     ]
    }
   ],
   "source": [
    "bert_sim = calculate_bert_similarity([\n",
    "    answer,\n",
    "    y_train.iloc[1][\"humanSpoiler\"]\n",
    "])\n",
    "print(\"BERT Similarity Score: \", bert_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b29ca1",
   "metadata": {},
   "source": [
    "### Q&A chunking with roberta-base BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "ba8a54a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(sentences, max_len=512, tokenizer=tokenizer_roberta):\n",
    "    chunks = []\n",
    "    chunk = []\n",
    "    token_count = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "        token_count += len(tokens)\n",
    "        \n",
    "        if token_count > max_len:\n",
    "            chunks.append(chunk)\n",
    "            chunk = [sentence]\n",
    "            token_count = len(tokens)\n",
    "        else:\n",
    "            chunk.append(sentence)\n",
    "    \n",
    "    if chunk:\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "a5c2f60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answer:  to explore an asteroid with the rocket, the Senate Appropriations Committee not only granted the request, but  to Mars. Eric Berger, a reporter at Ars Technica decried the move to build the rocket ahead of a destination. Cuts also came to NASA’s space technology funding, a $130 million bite meant to keep aloft earth-observing satellites run out of Goddard Space Flight Center, based in Mikulski’s state of Maryland. As Berger said, the motivation seems primarily to be keeping people employed. One problem with this approach is that you might end up with space rocket that does not have any space technology — a habitat module, deep-space propulsion, landers — to actually let astronauts voyage to asteroids, or the moon, or Mars. You might spend billions on workers who maintain the rocket you have built, and not have any money left over to build the interplanetary survival gear needed to ever get to Mars, the ultimate goal of the space agency. It is happened before, Garver said, with the the $100 billion International Space Station. We had to spend all the money just to get it built, with very little left for science. Scientists have long complained how little research takes place on the orbiting lab. NASA’s scientific advisory committee decided only this month to send a fact-finding group to the foundation that manages ISS research, after questioning its scientific productivity. NASA Watch / Keith Cowing / Via nasawatch.com  .gov ID: 8562049, ID: 8562045, 4. Mars Missions Are A Scam buzzfeed.com ID: 8562046</s>\n"
     ]
    }
   ],
   "source": [
    "context = x_train.iloc[1][\"targetParagraphs\"]\n",
    "query = x_train.iloc[1][\"targetTitle\"]\n",
    "\n",
    "sentences = nltk.sent_tokenize(context)\n",
    "chunks = chunk_text(sentences)\n",
    "\n",
    "answers = []\n",
    "for chunk in chunks:\n",
    "    context_chunk = \" \".join(chunk)\n",
    "    inputs = tokenizer_roberta(query, context_chunk, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "    \n",
    "    model_roberta.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model_roberta(**inputs)\n",
    "        start_index = torch.argmax(outputs.start_logits)\n",
    "        end_index = torch.argmax(outputs.end_logits)\n",
    "        \n",
    "        answer_tokens = inputs['input_ids'][0][start_index:end_index + 1]\n",
    "        answer = tokenizer_roberta.decode(answer_tokens)\n",
    "        answers.append(answer)\n",
    "\n",
    "final_answer = \" \".join(answers)\n",
    "print(f\"Final Answer: {final_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "ae7db1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Similarity Score:  0.8618836\n"
     ]
    }
   ],
   "source": [
    "bert_sim = calculate_bert_similarity([\n",
    "    final_answer,\n",
    "    y_train.iloc[1][\"humanSpoiler\"]\n",
    "])\n",
    "print(\"BERT Similarity Score: \", bert_sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eda_clickbait-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
