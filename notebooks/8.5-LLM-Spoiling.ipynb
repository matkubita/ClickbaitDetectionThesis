{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a27b681d-1350-426f-94a3-050e26207d8b",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to finetune BERT model for article spoiling task. <br>\n",
    "Use the conda env: eda_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9b0c10af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import accelerate\n",
    "import nltk\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# BERT model\n",
    "from transformers import RobertaTokenizer, RobertaForQuestionAnswering,  BertTokenizer, BertModel, logging, RobertaTokenizerFast\n",
    "import torch\n",
    "import nltk\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "af85298a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wojom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# VARIABLES\n",
    "nltk.download('punkt')\n",
    "RANDOM_STATE = 42 # Random state for reproducibility\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9333de52",
   "metadata": {},
   "source": [
    "### Loading data and splitting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7f80f950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3358, 6)\n"
     ]
    }
   ],
   "source": [
    "spoil_df = pd.read_csv(\"../data/spoiling_data.csv\", sep=\";\")\n",
    "print(spoil_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "618bff39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['targetTitle', 'targetParagraphs', 'humanSpoiler', 'spoiler', 'tags',\n",
       "       'spoilerPositions'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spoil_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0f13b5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(spoil_df.iloc[0][\"targetParagraphs\"]) != str:\n",
    "    spoil_df[\"targetParagraphs\"] = spoil_df[\"targetParagraphs\"].apply(ast.literal_eval)\n",
    "if type(spoil_df.iloc[0][\"targetParagraphs\"]) != str:\n",
    "    spoil_df[\"tags\"] = spoil_df[\"tags\"].apply(ast.literal_eval)\n",
    "    spoil_df[\"tags\"] = spoil_df[\"tags\"].apply(lambda x: x[0])\n",
    "if type(spoil_df.iloc[0][\"spoilerPositions\"]) == str:\n",
    "    spoil_df[\"spoilerPositions\"] = spoil_df[\"spoilerPositions\"].apply(ast.literal_eval)\n",
    "if type(spoil_df.iloc[0][\"spoiler\"]) != str:\n",
    "    spoil_df[\"spoiler\"] = spoil_df[\"spoiler\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6ac2db47",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    spoil_df.drop(columns=[\"humanSpoiler\", \"spoiler\"]), \n",
    "    spoil_df[[\"humanSpoiler\", \"spoiler\"]],\n",
    "    test_size=0.2, \n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "x_test, x_val, y_test, y_val = train_test_split(\n",
    "    x_test, \n",
    "    y_test,\n",
    "    test_size=0.5,  # 50% of the original x_test size for validation\n",
    "    random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d9ad74",
   "metadata": {},
   "source": [
    "### Loading models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "088751a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_bert_uncased = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model_bert_uncased = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def calculate_bert_similarity(texts):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        inputs = tokenizer_bert_uncased(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        outputs = model_bert_uncased(**inputs)\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().detach().numpy()\n",
    "        embeddings.append(embedding)\n",
    "    \n",
    "    similarity = np.dot(embeddings[0], embeddings[1]) / (np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1]))\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4d69d7fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>targetTitle</th>\n",
       "      <th>targetParagraphs</th>\n",
       "      <th>tags</th>\n",
       "      <th>spoilerPositions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2597</th>\n",
       "      <td>Why people like Edward Snowden say they will b...</td>\n",
       "      <td>Googles new messenger app is stirring up a deb...</td>\n",
       "      <td>passage</td>\n",
       "      <td>[[[2, 0], [2, 78]]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957</th>\n",
       "      <td>Why NASA Is Building An $18 Billion Rocket To ...</td>\n",
       "      <td>One piece of NASA’s massive new rocket NASA / ...</td>\n",
       "      <td>phrase</td>\n",
       "      <td>[[[8, 33], [8, 44]]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1926</th>\n",
       "      <td>Justin Bieber Kicked Out Of Hotel In Argentina...</td>\n",
       "      <td>Justin Bieber was allegedly kicked out of a ho...</td>\n",
       "      <td>passage</td>\n",
       "      <td>[[[1, 0], [1, 149]]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>Two players meet in No Man’s Sky, guess what h...</td>\n",
       "      <td>No Mans Sky is finally released in the UK and ...</td>\n",
       "      <td>phrase</td>\n",
       "      <td>[[[6, 112], [6, 135]]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2807</th>\n",
       "      <td>Ayvani Hope Perez Kidnapping: Teens Mom Was Ar...</td>\n",
       "      <td>Authorities in Georgia have revealed a new lin...</td>\n",
       "      <td>passage</td>\n",
       "      <td>[[[1, 88], [1, 133]]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            targetTitle  \\\n",
       "2597  Why people like Edward Snowden say they will b...   \n",
       "1957  Why NASA Is Building An $18 Billion Rocket To ...   \n",
       "1926  Justin Bieber Kicked Out Of Hotel In Argentina...   \n",
       "1991  Two players meet in No Man’s Sky, guess what h...   \n",
       "2807  Ayvani Hope Perez Kidnapping: Teens Mom Was Ar...   \n",
       "\n",
       "                                       targetParagraphs     tags  \\\n",
       "2597  Googles new messenger app is stirring up a deb...  passage   \n",
       "1957  One piece of NASA’s massive new rocket NASA / ...   phrase   \n",
       "1926  Justin Bieber was allegedly kicked out of a ho...  passage   \n",
       "1991  No Mans Sky is finally released in the UK and ...   phrase   \n",
       "2807  Authorities in Georgia have revealed a new lin...  passage   \n",
       "\n",
       "            spoilerPositions  \n",
       "2597     [[[2, 0], [2, 78]]]  \n",
       "1957    [[[8, 33], [8, 44]]]  \n",
       "1926    [[[1, 0], [1, 149]]]  \n",
       "1991  [[[6, 112], [6, 135]]]  \n",
       "2807   [[[1, 88], [1, 133]]]  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ddb752a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>humanSpoiler</th>\n",
       "      <th>spoiler</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1213</th>\n",
       "      <td>Your heart</td>\n",
       "      <td>heart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>Public = celebrities not government or society</td>\n",
       "      <td>Robin Roberts The Good Morning America anchor ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>In Detroit</td>\n",
       "      <td>Abraham Pearson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2478</th>\n",
       "      <td>Debating climate change with @TheScienceGuy an...</td>\n",
       "      <td>Meet the Press was singled out as failing to o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2295</th>\n",
       "      <td>So where are you going for college next year?</td>\n",
       "      <td>So, where are you going for college next year?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           humanSpoiler  \\\n",
       "1213                                         Your heart   \n",
       "321      Public = celebrities not government or society   \n",
       "2441                                         In Detroit   \n",
       "2478  Debating climate change with @TheScienceGuy an...   \n",
       "2295      So where are you going for college next year?   \n",
       "\n",
       "                                                spoiler  \n",
       "1213                                              heart  \n",
       "321   Robin Roberts The Good Morning America anchor ...  \n",
       "2441                                    Abraham Pearson  \n",
       "2478  Meet the Press was singled out as failing to o...  \n",
       "2295     So, where are you going for college next year?  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c395f1",
   "metadata": {},
   "source": [
    "### Q&A bm25 with roberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "78ff6466",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_roberta = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model_roberta = RobertaForQuestionAnswering.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "46484cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bm25(query, context):\n",
    "    sentences = nltk.sent_tokenize(context)\n",
    "    tokenized_corpus = [word_tokenize(doc.lower()) for doc in sentences]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "    tokenized_query = word_tokenize(query.lower())\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "    scores_idx = [[score, idx] for idx, score in enumerate(scores)]\n",
    "    scores_idx.sort(reverse=True, key=lambda x: x[0])\n",
    "    sentences = pd.DataFrame({\"sentences\": sentences})\n",
    "    important_sentences = sentences.iloc[[arr[1] for arr in scores_idx[:10]],:]\n",
    "    important_sentences_list = important_sentences[\"sentences\"].tolist()\n",
    "    return \" \".join(important_sentences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "86aaf839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spoiler(query, context, model=model_roberta, tokenizer=tokenizer_roberta):\n",
    "    inputs = tokenizer(query, context, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "    print(len(inputs[\"input_ids\"][0]))\n",
    "    print(inputs[\"input_ids\"][0])\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        start_index = torch.argmax(outputs.start_logits)\n",
    "        end_index = torch.argmax(outputs.end_logits)\n",
    "\n",
    "    answer = tokenizer.decode(inputs['input_ids'][0][start_index:end_index + 1])\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2ff0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[\"important_sentences\"] = x_train.apply(lambda x: calculate_bm25(x[\"targetTitle\"], x[\"targetParagraphs\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8dd17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[\"spoiler_bm25\"] = x_train.apply(lambda x: generate_spoiler(x[\"targetTitle\"], x[\"important_sentences\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "bf74e6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train.to_csv(\"../data/train_spoiler_bm25.csv\", sep=\";\")\n",
    "x_train = pd.read_csv(\"../data/train_spoiler_bm25.csv\", sep=\";\")\n",
    "x_train.set_index('Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "fa87c8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2686, 8)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.concat([x_train, y_train], axis=1)\n",
    "df_bm25 = df_train.loc[x_train[\"spoiler_bm25\"] != \"\"]\n",
    "df_bm25.shape # around 1000 rows with empty spoilers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d849b07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bm25[\"bert_sim\"] = df_bm25.apply(lambda x: calculate_bert_similarity([\n",
    "    x[\"spoiler_bm25\"],\n",
    "    x[\"spoiler\"]\n",
    "]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb62862",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bm25[\"bert_sim\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b29ca1",
   "metadata": {},
   "source": [
    "### Q&A chunking with roberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "ba8a54a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(sentences, max_len=512, tokenizer=tokenizer_roberta):\n",
    "    chunks = []\n",
    "    chunk = []\n",
    "    token_count = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "        token_count += len(tokens)\n",
    "        \n",
    "        if token_count > max_len:\n",
    "            chunks.append(chunk)\n",
    "            chunk = [sentence]\n",
    "            token_count = len(tokens)\n",
    "        else:\n",
    "            chunk.append(sentence)\n",
    "    \n",
    "    if chunk:\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a5c2f60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chuncked_spoiler(query, context, model=model_roberta, tokenizer=tokenizer_roberta):\n",
    "    sentences = nltk.sent_tokenize(context)\n",
    "    chunks = chunk_text(sentences)\n",
    "\n",
    "    answers = []\n",
    "    for chunk in chunks:\n",
    "        context_chunk = \" \".join(chunk)\n",
    "        inputs = tokenizer_roberta(query, context_chunk, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "        \n",
    "        model_roberta.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model_roberta(**inputs)\n",
    "            start_index = torch.argmax(outputs.start_logits)\n",
    "            end_index = torch.argmax(outputs.end_logits)\n",
    "            \n",
    "            answer_tokens = inputs['input_ids'][0][start_index:end_index + 1]\n",
    "            answer = tokenizer_roberta.decode(answer_tokens)\n",
    "            answers.append(answer)\n",
    "\n",
    "    final_answer = \" \".join(answers)\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36d940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[\"spoiler_chuncked\"] = x_train.apply(lambda x: generate_chuncked_spoiler(x[\"targetTitle\"], x[\"targetParagraphs\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7db1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_chuncked = y_train.loc[x_train[\"spoiler_chuncked\"] != \"\"]\n",
    "x_train_chuncked = x_train.loc[x_train[\"spoiler_chuncked\"] != \"\"]\n",
    "df_chuncked = pd.concat([x_train_chuncked, y_train_chuncked], axis=1)\n",
    "df_chuncked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643068bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chuncked[\"bert_sim\"] = df_chuncked.apply(lambda x: calculate_bert_similarity([\n",
    "    x[\"spoiler_chuncked\"],\n",
    "    x[\"spoiler\"]\n",
    "]), axis=1)\n",
    "df_chuncked[\"bert_sim\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d39c4a",
   "metadata": {},
   "source": [
    "### Roberta finetuning task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "68280871",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_roberta_fast = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "def head_tail_turncation(text, max_length=512, head_length=128, tail_length=382, tokenizer=tokenizer_roberta_fast):\n",
    "    tokenized = tokenizer(text, truncation=False, return_offsets_mapping=True)\n",
    "    input_ids = tokenized[\"input_ids\"]\n",
    "    attention_mask = tokenized[\"attention_mask\"]\n",
    "    offsets = tokenized[\"offset_mapping\"]\n",
    "\n",
    "    if len(input_ids) > max_length:\n",
    "        truncated_input_ids = input_ids[:head_length] + input_ids[-tail_length:]\n",
    "        truncated_attention_mask = attention_mask[:head_length] + attention_mask[-tail_length:]\n",
    "        truncated_offsets = offsets[:head_length] + offsets[-tail_length:]\n",
    "    else:\n",
    "        truncated_input_ids = input_ids\n",
    "        truncated_attention_mask = attention_mask\n",
    "        truncated_offsets = offsets\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": truncated_input_ids,\n",
    "        \"attention_mask\": truncated_attention_mask,\n",
    "        \"offset_mapping\": truncated_offsets,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7613e72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train\n",
    "title = x_train[\"targetTitle\"].iloc[0]\n",
    "paragraph = x_train[\"targetParagraphs\"].iloc[0]\n",
    "spoiler = y_train[\"spoiler\"].iloc[0]\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1fd0ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated = head_tail_turncation(title + paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062c0e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Truncated Input IDs:\", truncated[\"input_ids\"])\n",
    "print(\"Truncated Offset Mapping:\", truncated[\"offset_mapping\"])\n",
    "print(truncated[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74648dc5",
   "metadata": {},
   "source": [
    "## Finetuning T5Large\n",
    "\n",
    "32GB RAM memory is barely sufficient to start fine-tuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "204fcbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef37b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c670e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"Spoiler for this text: \"\n",
    "max_input_length = 800\n",
    "max_target_length = 128\n",
    "df_train = pd.concat([x_train, y_train], axis=1)\n",
    "df_test = pd.concat([x_test, y_test], axis=1)\n",
    "df_val = pd.concat([x_val, y_val], axis=1)\n",
    "df_train = Dataset.from_pandas(df_train)\n",
    "df_test = Dataset.from_pandas(df_test)\n",
    "df_val = Dataset.from_pandas(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d59336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [\n",
    "        f\"Extract spoiler from article. Title: {str(title)}; Tag: {str(tag)}; Content: {' '.join(paragraphs)}\"\n",
    "        for title, tag, paragraphs in zip(examples[\"targetTitle\"], examples[\"tags\"], examples[\"targetParagraphs\"])\n",
    "    ]\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=800,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    spoilers = [spoiler[0] for spoiler in examples[\"spoiler\"]]\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            spoilers,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=128,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58ae8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset_train = df_train.map(preprocess_function, batched=True)\n",
    "tokenized_dataset_test = df_test.map(preprocess_function, batched=True)\n",
    "tokenized_dataset_val = df_val.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db0afde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad87cbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "model_name = \"t5-large\"\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6686727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6268e872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "\n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3d7988",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_dataset_train,\n",
    "    eval_dataset=tokenized_dataset_val,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0c1e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eda_clickbait-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
